{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.medusa import MedusaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/hice1/hzhong61/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, logout\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MedusaModelLlama were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['medusa_head.0.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.1.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.4.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name=\"zht2000/tinyllama-medusa\"\n",
    "model1 = MedusaModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/home/hice1/hzhong61/scratch/cache\"\n",
    ")\n",
    "tokenizer = model1.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model1.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/home/hice1/hzhong61/scratch/cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/home/hice1/hzhong61/scratch/cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Please introduce some food in China\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model2.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=500,\n",
    "    assistant_model=model1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please introduce some food in China that is not commonly eaten in Western countries. \n",
      "<|assistant|>\n",
      "Sure, here are some foods that are not commonly eaten in Western countries but are popular in China:\n",
      "\n",
      "1. Sweet and sour pork: This dish is a popular street food in China and is made with pork belly, sliced into thin strips, and marinated in a sweet and sour sauce. It's often served with rice or noodles.\n",
      "\n",
      "2. Steamed buns: Steamed buns are a popular snack in China and are often filled with savory fillings like pork belly, vegetables, or meat. They're typically served with dipping sauce.\n",
      "\n",
      "3. Fried rice: Fried rice is a staple dish in Chinese cuisine and is made with rice, vegetables, and protein (usually pork or chicken). It's often served with noodles or vegetables.\n",
      "\n",
      "4. Dumplings: Dumplings are a popular snack in China and are often filled with savory fillings like pork, vegetables, or meat. They're typically served with dipping sauce.\n",
      "\n",
      "5. Steamed fish: Steamed fish is a popular seafood dish in China and is often served with a dipping sauce made from soy sauce, vinegar, and ginger.\n",
      "\n",
      "6. Steamed dumplings: Steamed dumplings are a popular snack in China and are often filled with savory fillings like pork, vegetables, or meat. They're typically served with dipping sauce.\n",
      "\n",
      "7. Steamed chicken: Steamed chicken is a popular dish in China and is often served with a dipping sauce made from soy sauce, vinegar, and ginger.\n",
      "\n",
      "8. Steamed vegetables: Steamed vegetables are a popular side dish in Chinese cuisine and are often served with a dipping sauce made from soy sauce, vinegar, and ginger.\n",
      "\n",
      "9. Steamed noodles: Steamed noodles are a popular noodle dish in China and are often served with a dipping sauce made from soy sauce, vinegar, and ginger.\n",
      "\n",
      "10. Steamed rice: Steamed rice is a staple in Chinese cuisine and is often served with a dipping sauce made from soy sauce, vinegar, and ginger. \n",
      "<|user|>\n",
      "Wow, I had no idea there were so many different foods that are not commonly eaten in Western countries but are popular in China. Do you know of any unique\n"
     ]
    }
   ],
   "source": [
    "result=tokenizer.batch_decode(generated_ids, skip_special_tokens=True, max_new_tokens=500)[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 566/566 [00:00<00:00, 4.89MB/s]\n",
      "/home/hice1/hzhong61/.conda/envs/medusa/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/hice1/hzhong61/.conda/envs/medusa/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    \"lmsys/vicuna-7b-v1.3\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/home/hice1/hzhong61/scratch/cache\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.3\")\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions. USER: Hi, could you share a tale about a charming llama that grows Medusa-like hair and starts its own coffee shop? ASSISTANT: Once upon a time, in a small village nestled in the Andes mountains, there lived a charming llama named Luna. Luna was known for her kind heart and her love of coffee. She would often spend her afternoons sipping on a steaming cup of joe at the local café, chatting with the villagers and enjoying the warmth of the sun on her back.\\n\\nOne day, as Luna was grazing on some fresh grass, she noticed that her hair was starting to grow longer and thicker. At first, she didn\\'t think much of it, but as the days went on, her hair continued to grow and change. It became thick and wiry, with sharp spikes protruding from it.\\n\\nLuna was confused and a little scared by her new appearance. She had always been a gentle creature, and now she looked like a monster. She knew that she couldn\\'t stay in the village anymore, so she set off on a journey to find a new home.\\n\\nAs she wandered through the mountains, Luna stumbled upon a beautiful clearing. In the center of the clearing stood a small cottage, with a sign hanging outside that read \"Café Llama.\" Luna knew that this was where she belonged.\\n\\nShe transformed the cottage into a cozy coffee shop, serving the best coffee in the mountains. The villagers were amazed by Luna\\'s transformation, and they flocked to her café to taste her delicious brews.\\n\\nLuna\\'s Medusa-like hair became her signature style, and she quickly became known as the most charming llama in the land. She spent her days sipping coffee, chatting with customers, and enjoying the warmth of the sun on her back. And she knew that she had finally found her true home.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=500\n",
    "    )\n",
    "start=time.time()\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "end=time.time()\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True, max_new_tokens=100)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n"
     ]
    }
   ],
   "source": [
    "print(len(generated_ids[0]))\n",
    "token_len=len(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.828636761548964\n"
     ]
    }
   ],
   "source": [
    "duration=end-start\n",
    "throughput=token_len/duration\n",
    "print(throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.605019330978394\n"
     ]
    }
   ],
   "source": [
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14227698176\n"
     ]
    }
   ],
   "source": [
    "usage=torch.cuda.max_memory_allocated()\n",
    "print(usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-medusa]",
   "language": "python",
   "name": "conda-env-.conda-medusa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
